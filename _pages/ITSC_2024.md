---
layout: page
title: ITSC 2024
permalink: /ITSC_2024/
subtitle:

# profile:
#   align: 
#   image: 
#   image_circular: false # crops the image to make it circular
#   address: 

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---


<!-- 
Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

----------
### About the Workshop

<!-- **Note for Submission:** In light of the extension of final decision release for the WACV 2024 main conference, we decided to extend our submission deadline to **October 26th, 2023**. -->
The 2nd Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD) at [ITSC 2024](https://ieee-itsc.org/2024/) aims to bring together professionals from academia and industry to explore the application of large language and vision models in autonomous driving. As part of this initiative, the 2nd LLVM-AD workshop launches an open dataset challenge for real-world traffic understanding.

<!-- ----------

### Important Dates

- Paper Submission Deadline: TBD
- Author Notification: TBD
- Camera-ready Papers Deadline: TBD -->

----------

### Keynote Speakers
<div class="row projects pt-1 pb-1">
      <div class="col-sm-4">
          {% include people.html name="Manmohan Chandraker" affiliation="UCSD & NEC Lab America" url="https://www.nec-labs.com/research/media-analytics/people/manmohan-chandraker/" img="assets/img/itsc_headshots/manmohan.jpg" %}
      </div>
      <div class="col-sm-4">
          {% include people.html name="Long Chen" affiliation="Wayve" url="https://long.ooo/" img="assets/img/itsc_headshots/long_chen.png" %}
      </div>
  </div>

----------

### Workshop Recording

<!-- https://www.youtube.com/watch?v=zvrsS3JjThE&t=4s -->

<iframe width="560" height="315" src="https://www.youtube.com/embed/zvrsS3JjThE?si=9qeL7ms0QPHmmrwg" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" referrerpolicy="strict-origin-when-cross-origin" allowfullscreen></iframe>

----------

### Organizers

<div class="row row-cols-2 projects pt-3 pb-3">
  {% include people_horizontal.html name="Yunsheng Ma" affiliation="Purdue University" url="https://ysma.me/" img="assets/img/itsc_headshots/linkedin-company-logo-yunsheng.jpg" %}
  {% include people_horizontal.html name="Can Cui" affiliation="Purdue University" url="https://cancui19.github.io/" img="assets/img/itsc_headshots/linkedin-company-logo-can_cui.jpg" %}
  {% include people_horizontal.html name="Xu Cao" affiliation="University of Illinois Urbana Champaign" url="https://www.irohxucao.com/" img="assets/img/itsc_headshots/linkedin-company-logo-xu_cao.jpg" %}
  {% include people_horizontal.html name="Wenqian Ye" affiliation="University of Virginia" url="https://wenqian-ye.github.io" img="assets/img/itsc_headshots/linkedin-company-logo-wenqian.jpeg" %}
  {% include people_horizontal.html name="SungYeon Park" affiliation="Purdue University" url="https://sypark574.wixsite.com/website" img="assets/img/itsc_headshots/linkedin-company-logo-sungyeon.webp" %}
  {% include people_horizontal.html name="Yi Yang" affiliation="KTH" url="https://www.kth.se/profile/yiya?l=en" img="assets/img/itsc_headshots/linkedin-company-logo-yi_yang.jpeg" %}
  {% include people_horizontal.html name="Amr Abdelraouf" affiliation="Toyota Motor North America" url="https://scholar.google.com/citations?user=kWR3NRUAAAAJ&hl=en" img="assets/img/itsc_headshots/linkedin-company-logo-amr.jpeg" %}
  {% include people_horizontal.html name="Jong-Chyi Su" affiliation="Google" url="https://jongchyisu.github.io/" img="assets/img/itsc_headshots/linkedin-company-logo-jong-chyi.jpg" %}
  {% include people_horizontal.html name="Zhengzhong Tu" affiliation="Texas A&M University" url="https://vztu.github.io/" img="assets/img/itsc_headshots/linkedin-company-logo-zhengzhong.jpeg"%}
  {% include people_horizontal.html name="Jiachen Li" affiliation="University of California, Riverside" url="https://jiachenli94.github.io/" img="assets/img/itsc_headshots/jiachen.jpg"%}
  {% include people_horizontal.html name="Ziran Wang" affiliation="Purdue University" url="https://ziranw.github.io/" img="assets/img/itsc_headshots/linkedin-company-logo-ziran.jpg" %}
</div>


----------

### Keynote Talks

#### Towards Scalable Autonomy (Manmohan Chandraker)
 
**Abstract:**
While autonomous driving has made large strides over the past decade to be deployed in practice today, scaling it across a diversity of conditions and behaviors remains expensive. This talk explores the possibility of recent advances in large language models (LLMs) and computer vision foundational models allowing the development of more scalable driving, simulation and devops stacks for autonomous mobility. These include perception and planning that leverage language guidance, photorealistic simulation of safety-critical scenarios with neural rendering and controllable diffusion, automated devops based on vision-language understanding and a look ahead to agentic LLMs that automate complex autonomy workflows.
 
**Bio:**
Manmohan Chandraker is a professor in the CSE department of the University of California, San Diego and leads computer vision research at NEC Labs. His research interests are in vision, learning and graphics, with applications to autonomous driving and augmented reality. His works have been recognized with best paper awards at CVPR, ICCV and ECCV, the NSF CAREER Award, Qualcomm and Google Research Awards. He serves on NSF panels on vision, learning and robotics and on senior program committees at CVPR, ICCV, ECCV, AAAI, NeurIPS and ICLR. 

#### Large Language and Vision Models for Autonomous Driving (Long Chen)

**Bio:** 
Long Chen is a distinguished research lead, with a proven track record in developing disruptive AI technologies. He currently holds the position of Staff Scientist at Wayve, where he is at the forefront of building vision-language-action (VLA) models for the next wave of autonomous driving, such as Driving-with-LLMs and LINGO. Previously, he was a research engineer at Lyft Level 5, where he led the data-driven planning models from crowd-sourced data for Lyftâ€™s self-driving cars. His extensive experience also includes applying AI technologies in various domains such as mixed reality, surgical robots, and healthcare.

----------
{% comment %}
* [Yunsheng Ma](https://ysma.me/), Purdue University.
* [Can Cui](https://cancui19.github.io/), Purdue University.
* [Xu Cao](https://www.irohxucao.com/), University of Illinois.
* [Wenqian Ye](wenqian-ye.github.io), University of Virginia.
* [SungYeon Park](https://sypark574.wixsite.com/website), Purdue University.
* [Yi Yang](https://www.kth.se/profile/yiya?l=en), KTH.
* [Amr Abdelraouf](https://scholar.google.com/citations?user=kWR3NRUAAAAJ&hl=en), Toyota Motor North America.
* [Jong-Chyi Su](https://jongchyisu.github.io/), Google.
* [Zhengzhong Tu](https://vztu.github.io/), Texas A&M University.
* [Jiachen Li](https://jiachenli94.github.io/), University of California, Riverside.
* [Ziran Wang](https://ziranw.github.io/), Purdue University.
* 
{% endcomment %}
