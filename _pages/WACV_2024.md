---
layout: page
title: WACV 2024
permalink: /wacv_2024/
subtitle:
# nav: true
# nav_order: 5

# profile:
#   align: 
#   image: 
#   image_circular: false # crops the image to make it circular
#   address: 

news: false  # includes a list of news items
latest_posts: false  # includes a list of the newest posts
selected_papers: false # includes a list of papers marked as "selected={true}"
social: false  # includes social icons at the bottom of the page
---


<!-- 
Write your biography here. Tell the world about yourself. Link to your favorite [subreddit](http://reddit.com). You can put a picture in, too. The code is already in, just name your picture `prof_pic.jpg` and put it in the `img/` folder.

Put your address / P.O. box / other info right below your picture. You can also disable any of these elements by editing `profile` property of the YAML header of your `_pages/about.md`. Edit `_bibliography/papers.bib` and Jekyll will render your [publications page](/al-folio/publications/) automatically.

Link to your social media connections, too. This theme is set up to use [Font Awesome icons](http://fortawesome.github.io/Font-Awesome/) and [Academicons](https://jpswalsh.github.io/academicons/), like the ones below. Add your Facebook, Twitter, LinkedIn, Google Scholar, or just disable all of them. -->

<!-- ### About LLVM-AD -->
The 1st [WACV](https://wacv2024.thecvf.com/) Workshop on **Large Language and Vision Models for Autonomous Driving (
LLVM-AD)** seeks to bring together academia and industry professionals in a collaborative exploration of applying large
language and vision models to autonomous driving. Through a half-day in-person event, the workshop will showcase regular
and demo paper presentations and invited talks from famous researchers in academia and industry. Additionally, LLVM-AD
will launch two open-source real-world traffic language understanding datasets, catalyzing practical advancements. The
workshop will host two challenges based on this dataset to assess the capabilities of language and computer vision
models in addressing autonomous driving challenges.

**Note for Benchmark:** The workshop challenge will be maintained in the long term, and even after the workshop concludes, we will continue to welcome submissions of new results on the datasets. We will also update the benchmark accordingly.

<!-- **Note for Submission:** In light of the extension of final decision release for the WACV 2024 main conference, we decided to extend our submission deadline to **October 26th, 2023**. -->

----------

### Workshop Recording

<iframe width="540" height="350"
src="https://www.youtube.com/embed/WNaJPyVNPdw">
</iframe>



----------

### Important Dates

- Paper Submission Deadline: ~~October 23rd, 2023~~ <span style="color:red">October 26th, 2023</span>
- Author Notification: November 13th, 2023
- Camera-ready Papers Deadline: November 19th, 2023

----------

### Invited Speakers
<div style="overflow-x: auto;">
  <table style="width:75%">
    <tr>
      <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/zhen_li.png" height="150"></td>
      <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/oleg.png" height="150"></td>
      <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/yu_huang.jpg" height="150"></td>
    </tr><tr>
      <td style="text-align:center"><a href="https://mypage.cuhk.edu.cn/academics/lizhen/">Dr. Zhen Li</a> <br>Assistant Professor, CUHKSZ</td>
      <td style="text-align:center"><a href="https://www.linkedin.com/in/oleg-sinavski/">Dr. Oleg Sinavski</a> <br> Principal Applied Scientist, Wayve</td>
      <td style="text-align:center"><a href="https://www.linkedin.com/in/yuhuang/">Dr. Yu Huang</a> <br> CEO and Chief Scientist, roboraction.ai</td>
    </tr> 


  </table>
</div>


----------

### Organizers

<!-- <table style="width:75%">
  <tr>
    <td style="text-align:center"><img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=6A1yEFMAAAAJ&citpid=1" height="200"></td>
    <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Kun_Tang.jpg" height="200"></td>
    <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Zhipeng_Cao.jpg" height="200"></td>
    <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Xu_Cao.jpg" height="200"></td>
    <td style="text-align:center"><img src="https://purduedigitaltwin.github.io/assets/images/people/yunsheng.jpg" height="200"></td>
  </tr><tr>
    <td style="text-align:center"><a href="">Chao Zheng</a> <br> Tencent</td>
    <td style="text-align:center"><a href="">Kun Tang</a> <br>Tencent</td>
    <td style="text-align:center"><a href="">Zhipeng Cao</a> <br>Tencent</td>
    <td style="text-align:center"><a href="https://www.linkedin.com/in/irohxu/">Xu Cao</a> <br>PediaMed AI & University of Illinois Urbana-Champaign</td>
    <td style="text-align:center"><a href="https://maysonma.github.io/">Yunsheng Ma</a> <br>Purdue University</td>
  </tr>
  <tr>
    <td style="text-align:center"><img src="https://purduedigitaltwin.github.io/assets/images/people/can.jpg" height="200"></td>
    <td style="text-align:center"><img src="https://wenqian-ye.github.io/images/selfie.jpeg" height="200"></td>
    <td style="text-align:center"><img src="https://ziranw.github.io/Attachments/Ziran_Headshot.jpg" height="200"></td>
    <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Shawn_Mei.png" height="200"></td>
    <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Tong_Zhou.jpg" height="200"></td>
  </tr><tr>
    <td style="text-align:center"><a href="https://cancui19.github.io/">Can Cui</a> <br> Purdue University</td>
    <td style="text-align:center"><a href="https://wenqian-ye.github.io/">Wenqian Ye</a> <br> PediaMed AI & University of Virginia </td>
    <td style="text-align:center"><a href="https://ziranw.github.io">Ziran Wang</a> <br>Purdue University</td>
    <td style="text-align:center"><a href="">Shawn Mei</a> <br> Tencent</td>
    <td style="text-align:center"><a href="">Tong Zhou</a> <br> Tencent</td>
  </tr>
</table> -->

<div style="overflow-x: auto;">
  <table style="width:75%">
    <tr>
      <td style="text-align:center"><img src="https://scholar.googleusercontent.com/citations?view_op=medium_photo&user=6A1yEFMAAAAJ&citpid=1" height="100"></td>
      <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Kun_Tang.jpg" height="100"></td>
      <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Zhipeng_Cao.jpg" height="100"></td>
      <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Xu_Cao.jpg" height="100"></td>
      <td style="text-align:center"><img src="https://purduedigitaltwin.github.io/assets/images/people/yunsheng.jpg" height="100"></td>
    </tr>
    <tr>
      <td style="text-align:center"><a href="">Chao Zheng</a> <br> Tencent</td>
      <td style="text-align:center"><a href="">Kun Tang</a> <br>Tencent</td>
      <td style="text-align:center"><a href="">Zhipeng Cao</a> <br>Tencent</td>
      <td style="text-align:center"><a href="https://www.linkedin.com/in/irohxu/">Xu Cao</a> <br>UIUC</td>
      <td style="text-align:center"><a href="https://maysonma.github.io/">Yunsheng Ma</a> <br>Purdue</td>
    </tr>
    <tr>
      <td style="text-align:center"><img src="https://purduedigitaltwin.github.io/assets/images/people/can.jpg" height="100"></td>
      <td style="text-align:center"><img src="https://wenqian-ye.github.io/images/selfie.jpeg" height="100"></td>
      <td style="text-align:center"><img src="https://ziranw.github.io/Attachments/Ziran_Headshot.jpg" height="100"></td>
      <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Shawn_Mei.png" height="100"></td>
      <td style="text-align:center"><img src="https://raw.githubusercontent.com/LLVM-AD/llvm-ad.github.io/main/assets/img/Tong_Zhou.jpg" height="100"></td>
    </tr>
    <tr>
      <td style="text-align:center"><a href="https://cancui19.github.io/">Can Cui</a> <br> Purdue</td>
      <td style="text-align:center"><a href="https://wenqian-ye.github.io/">Wenqian Ye</a> <br> UVA </td>
      <td style="text-align:center"><a href="https://ziranw.github.io">Ziran Wang</a> <br>Purdue</td>
      <td style="text-align:center"><a href="">Shawn Mei</a> <br> Tencent</td>
      <td style="text-align:center"><a href="">Tong Zhou</a> <br> Tencent</td>
    </tr>
  </table>
</div>
----------

### Accepted Papers
**Summary of the 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD):** \[[Arxiv](https://arxiv.org/abs/2311.12320), [GitHub](https://github.com/IrohXu/Awesome-Multimodal-LLM-Autonomous-Driving)\]

**ðŸŽ‰ We would like to congrate the following papers for being accepted to LLVM-AD 2024!**

- [Drive as You Speak: Enabling Human-Like Interaction with Large Language Models in Autonomous Vehicles](https://arxiv.org/abs/2309.10228)     

- [Drive Like a Human: Rethinking Autonomous Driving with Large Language Models](https://arxiv.org/abs/2307.07162)       

- [A Game of Bundle Adjustment - Learning Efficient Convergence](https://arxiv.org/abs/2308.13270) Accepted as a tech report for ICCV 2023 Paper          

- VLAAD: Vision and Language Assistant for Autonomous Driving      

- A Safer Vision-based Autonomous Planning System for Quadrotor UAVs with Dynamic Obstacle Trajectory Prediction and Its Application with LLMs     

- [Human-Centric Autonomous Systems With LLMs for User Command Reasoning](https://arxiv.org/abs/2311.08206)       

- NuScenes-MQA: Integrated Evaluation of Captions and QA for Autonomous Driving Datasets using Markup Annotations       

- Latency Driven Spatially Sparse Optimization for Multi-Branch CNNs for Semantic Segmentation       

- LIP-Loc: LiDAR Image Pretraining for Cross-Modal Localization  


----------

### Challenge Organization Committee

- Chao Zheng (Tencent)
- Kun Tang (Tencent)
- Zhipeng Cao (Tencent)
- Tong Zhou (Tencent)
- Erlong Li (Tencent)
- Ao Liu (Tencent)
- Shengtao Zou (Tencent)
- Xinrui Yan (Tencent)
- Shawn Mei (Tencent)
- Yunsheng Ma (Purdue University)
- Can Cui (Purdue University)
- Ziran Wang (Purdue University)
- Yang Zhou (New York University)
- Kaizhao Liang (SambaNova Systems)
- Wenqian Ye (PediaMed AI & University of Virginia)
- Xu Cao (PediaMed AI & University of Illinois Urbana-Champaign)

----------

### Program Committee

- Erlong Li (Tencent)
- Ao Liu (Tencent)
- Shengtao Zou (Tencent)
- Xinrui Yan (Tencent)
- Yang Zhou (New York University)
- Kaizhao Liang (SambaNova Systems)
- Tianren Gao (SambaNova Systems)
- Kuei-Da Liao (SambaNova Systems)
- Shan Bao (University of Michigan)
- Xuhui Kang (University of Virginia)
- Sean Sung-Wook Lee (University of Virginia)
- Amr Abdelraouf (Toyota Motor North America)
- Jianguo Cao (PediaMed AI)
- Jintai Chen (University of Illinois Urbana-Champaign)



### Citation    

If the workshop and the survey inspire you, please consider citing our work:    

```
@misc{cui2023survey,
      title={A Survey on Multimodal Large Language Models for Autonomous Driving}, 
      author={Can Cui and Yunsheng Ma and Xu Cao and Wenqian Ye and Yang Zhou and Kaizhao Liang and Jintai Chen and Juanwu Lu and Zichong Yang and Kuei-Da Liao and Tianren Gao and Erlong Li and Kun Tang and Zhipeng Cao and Tong Zhou and Ao Liu and Xinrui Yan and Shuqi Mei and Jianguo Cao and Ziran Wang and Chao Zheng},
      year={2023},
      eprint={2311.12320},
      archivePrefix={arXiv},
      primaryClass={cs.AI}
}
```