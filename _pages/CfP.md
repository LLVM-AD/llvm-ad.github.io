---
layout: page
permalink: /call_for_papers/
title: Call for Papers
description:
nav: false
nav_order: 2
---


The LLVM-AD Committee is accepting research paper submissions on various topics, including but not limited to:
- *Vision or Language Models in Autonomous Driving*
- *Multimodal Motion Planning and Prediction*
- *New Dataset for Language or Vision Models in Autonomous Driving*
- *Semantics and Scene Understanding in Autonomous Driving*
- *Language-Driven Sensor and Traffic Simulation*
- *Domain Adaptation and Transfer Learning in Autonomous Driving*
- *Multi-Modal Fusion for Autonomous Driving*
- *Other Applications of Language or Vision Models for Driving*


LLVM-AD accepts papers in PDF format via Papercept/Paperplaza platform, maximum 8 (expected 6) pages including references. Accepted papers will be included in ITSC proceedings and will be published by the IEEE Xplore. Papers with highest quality will receive best paper awards.


<!-- For now, this page is assumed to be a static description of your courses. You can convert it to a collection similar to `_projects/` so that you can have a dedicated page for each course.

Organize your courses by years, topics, or universities, however you like! -->

<!-- The LLVM-AD committee invites papers that will undergo the standard peer review process. We accept submissions through our **[CMT](https://cmt3.research.microsoft.com/LLVMAD2024)**. If accepted, the workshop papers will be published in IEEE Xplore as WACV 2024 Workshop Proceedings (See previous WACV proceedings format [here](https://openaccess.thecvf.com/WACV2023_workshops/menu)) and will be indexed separately from the main conference proceedings. The papers submitted to the workshop should follow the same formatting requirements as the main conference. Program topics include:

- *Vision-Language Models in Autonomous Systems*

- *Large Language Models for Autonomous Driving Applications and Map Systems*

- *Foundation Vision Models for Autonomous Driving*

- *Generative AI for Autonomous Driving and Map Systems*

- *Semantics and Scene Understanding in Autonomous Driving*

- *Human-Vehicle Interaction*

- *Domain Adaptation and Transfer Learning in Autonomous Driving*

- *Safety in Autonomous Driving*

- *Interpretable AI in Autonomous Driving*

- *Trustworthy Autonomous Vehicles* -->

<!-- The first LLVM-AD workshop invites submissions that contribute to the progression of LLVM within the domain of autonomous driving. We are particularly interested in bridging the gap between the rich image and language data found within the context of autonomous driving. Our primary areas of interest are: a). Traffic Scene Understanding enhanced by LLVMs and 
b). Human-Vehicle Interactions driven by LLVMs. The detail will be released soon.


If accepted, the workshop papers will be published in IEEE Xplore as WACV 2024 Workshop Proceedings and will be indexed separately from the main conference proceedings. The papers submitted to the workshop should follow the same formatting requirements as the main conference. -->

<!-- LLVM-AD accepts both **regular papers (6-8 pages)** and **demo papers (2-4 pages)**. The paper with the highest quality will receive the best paper award. 

**Long Papers**: Manuscripts of upto **8 pages**. Submissions are expected to follow the same guidelines as those for WACV's main conference papers. The best paper elected by the committee will have the opportunity to receive outstanding paper award and **\$300** in recognition of their outstanding contributions.

**Demo papers**: Manuscripts of upto **4 pages**. Demo papers should also include 1 additional page for references, maintaining the same formatting and guidelines. The main objective of these demo papers within the workshop is to foster the exchange of novel concepts and spotlight the practical utility of LLMs in the context of autonomous driving. Accepted demo papers will be included into the workshop proceedings. Authors of the most exceptional demo papers will have chance to be invited to Tencent Map T Lab for an official summer internship. -->

