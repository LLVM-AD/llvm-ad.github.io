<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title>Challenges | LLVM-AD Workshop @ WACV 2024</title> <meta name="author" content=" "> <meta name="description" content="The 1st WACV Workshop on Large Language and Vision Models for Autonomous Driving (LLVM-AD). "> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link href="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/css/bootstrap.min.css" rel="stylesheet" integrity="sha256-DF7Zhf293AJxJNTmh5zhoYYIMs2oXitRfBjY+9L//AY=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.css"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/@fortawesome/fontawesome-free@6.4.0/css/all.min.css" integrity="sha256-HtsXJanqjKTc8vVQjO4YMhiqFoXkfBsjBWcX91T1jr8=" crossorigin="anonymous"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/academicons@1.9.1/css/academicons.min.css" integrity="sha256-i1+4qU2G2860dGGIOJscdC30s9beBXjFfzjWLjBRsBg=" crossorigin="anonymous"> <link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/github.css" media="" id="highlight_theme_light"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://llvm-ad.github.io/challenge/"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jwarby/jekyll-pygments-themes@master/native.css" media="none" id="highlight_theme_dark"> <script src="/assets/js/theme.js?96d6b3e1c3604aca8b6134c7afdd5db6"></script> <script src="/assets/js/dark_mode.js?9b17307bb950ffa2e34be0227f53558f"></script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/">LLVM-AD Workshop @ WACV 2024</a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">LLVM-AD</a> </li> <li class="nav-item "> <a class="nav-link" href="/call_for_papers/">Call for Papers</a> </li> <li class="nav-item active"> <a class="nav-link" href="/challenge/">Challenges<span class="sr-only">(current)</span></a> </li> <li class="nav-item "> <a class="nav-link" href="/schedule/">Schedule</a> </li> <li class="nav-item "> <a class="nav-link" href="/previous_workshop/">Previous Workshop</a> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5"> <div class="post"> <header class="post-header"> <h1 class="post-title">Challenges</h1> <p class="post-description"></p> </header> <article> <p>Two open-source datasets are provided for this workshop. The first dataset focuses on extracting traffic language from HD maps to aid in traffic scenario comprehension through LLVMs. The second dataset aims to categorize textbased driver commands to improve human-vehicle language understanding. While using the datasets is recommended, it is <strong>not mandatory</strong> for participation.</p> <hr> <h3 id="challenge-1-maplm-a-large-scale-vision-language-dataset-for-map-and-traffic-scene-understanding">Challenge 1: MAPLM: A Large-Scale Vision-Language Dataset for Map and Traffic Scene Understanding</h3> <p><strong><a href="https://drive.google.com/drive/folders/1cqFjBH8MLeP6nKFM0l7oV-Srfke-Mx1R?usp=sharing" rel="external nofollow noopener" target="_blank">Data Download</a>; <a href="https://github.com/LLVM-AD/MAPLM" rel="external nofollow noopener" target="_blank">Git Repo</a></strong></p> <p>Tencent Maps HD Map T.Lab, in collaboration with the University of Illinois at Urbana-Champaign, Purdue University, and the University of Virginia, have launched the industry’s first multimodal language+vision (point cloud BEV+panoramic images) traffic scenario understanding dataset: MAPLM. MAPLM provides abundant road scenario images complemented with multi-level scene description data, aiding models in navigating complex and varied traffic environments.</p> <h4 id="data-download">Data Download:</h4> <p>Put the maplm v0.1.z01, maplm v0.1.z02, maplm v0.1.z03, maplm v0.1.zip into one directory then run the following command to unzip the dataset.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>zip <span class="nt">-s</span> 0  maplm_v0.1.zip <span class="nt">--out</span> combine.zip
unzip combine.zip
</code></pre></div></div> <h4 id="scene-of-maplm">Scene of MAPLM：</h4> <p>MAPLM offers a variety of traffic scenarios, including highways, expressways, city roads, and rural roads, along with detailed intersection scenes. Each frame of data includes two components:</p> <ul> <li> <strong>Point Cloud BEV:</strong> A projection image of 3D point cloud viewed from the BEV perspective with clear visuals and high resolution.</li> <li> <strong>Panoramic Images:</strong> High-resolution photographs captured from front, left-rear, and right-rear angles by a wide-angle camera.</li> </ul> <h4 id="annotations">Annotations：</h4> <ul> <li> <strong>Feature-level:</strong> Lane lines, ground signs, stop lines, intersection areas, etc.</li> <li> <strong>Lane-level:</strong> Lane types, directions of traffic, turn categories, etc.</li> <li> <strong>Road-level:</strong> Scene types, road data quality, intersection structures, etc.</li> </ul> <h4 id="data-display">Data Display：</h4> <p>Point Cloud BEV image + 3 panoramic photos. Note: Panoramic images are 4096*3000 portrait shots. The image below is only a cropped sample.<br></p> <div class="msg_desc"> <img style="max-width:100%;overflow:hidden;" src="https://raw.githubusercontent.com/LLVM-AD/MAPLM/main/figures/example1.png" alt=""> </div> <p><br></p> <h4 id="label-display">Label Display：</h4> <p>The image below illustrates one frame’s annotation information, encompassing three parts: road-level information (in red font), lane-level information (yellow geometric lines + orange font), and intersection data (blue polygons + blue font).<br></p> <div class="msg_desc"> <img style="max-width:100%;overflow:hidden;" src="https://raw.githubusercontent.com/LLVM-AD/MAPLM/main/figures/example2.png" alt=""> </div> <p><br></p> <hr> <h3 id="challenge-2-in-cabin-user-command-understanding-ucu">Challenge 2: In-Cabin User Command Understanding (UCU)</h3> <p><strong><a href="https://github.com/LLVM-AD/ucu-dataset/blob/main/ucu.csv" rel="external nofollow noopener" target="_blank">Data Download</a>; <a href="https://github.com/LLVM-AD/ucu-dataset" rel="external nofollow noopener" target="_blank">Code for Evaluation</a></strong></p> <p>The future of autonomous vehicles is not only to transport passengers from point A to point B, but also to adapt to their needs and preferences. Large Language Models (LLMs) have provided an opportunity to advance the state-of-the-art of this vision. This dataset focuses on understanding user commands in the context of autonomous vehicles.</p> <h4 id="data">Data</h4> <p>The dataset contains 1,099 labeled commands. Each command is a sentence that describes a user’s request to the vehicle. For example, “Overtake the car in front of me”. The objective is to answer the following 8 Yes/No questions about executing the command in an autonomous vehicle:</p> <ol> <li>Is the <code class="language-plaintext highlighter-rouge">external perception system</code> required?</li> <li>Is <code class="language-plaintext highlighter-rouge">in-cabin monitoring</code> required?</li> <li>Is <code class="language-plaintext highlighter-rouge">localization</code> required?</li> <li>Is <code class="language-plaintext highlighter-rouge">vehicle control</code> required?</li> <li>Is the <code class="language-plaintext highlighter-rouge">entertainment system</code> required?</li> <li>Is <code class="language-plaintext highlighter-rouge">user personal data</code> required?</li> <li>Is <code class="language-plaintext highlighter-rouge">external network access</code> required?</li> <li>Is there a possibility of <code class="language-plaintext highlighter-rouge">violating traffic laws</code>?</li> </ol> <p>The following definitions explain the terms used in the questions:</p> <ol> <li> <code class="language-plaintext highlighter-rouge">external perception system</code> refers to the sensors and software that allow the autonomous vehicle to perceive its surroundings. It typically includes cameras, lidar, radar, and other sensors to detect objects, pedestrians, other vehicles, road conditions, and traffic signs/signals.</li> <li> <code class="language-plaintext highlighter-rouge">in-cabin monitoring</code> involves cameras, thermometers, or other sensors placed inside the vehicle’s cabin to monitor the state of occupants and other conditions.</li> <li> <code class="language-plaintext highlighter-rouge">localization</code> is the ability of the vehicle to determine its precise position in a given environment. Typically done using a combination of GPS, sensors, and high-definition maps.</li> <li> <code class="language-plaintext highlighter-rouge">vehicle control</code> refers to the system that makes the driving decisions and physically controls the vehicle movements, such as steering, acceleration, braking, and signaling.</li> <li> <code class="language-plaintext highlighter-rouge">entertainment system</code> is the multimedia system in a vehicle, which can include radio, music players, video displays, and other entertainment features.</li> <li> <code class="language-plaintext highlighter-rouge">user personal data</code> is the information relating to an identified or identifiable individual, such as contact details, preferences, travel history, etc.</li> <li> <code class="language-plaintext highlighter-rouge">external network access</code> is the ability of the vehicle’s systems to connect to external networks, such as the internet or cloud services.</li> <li> <code class="language-plaintext highlighter-rouge">violating traffic laws</code> refers to any action performed by the vehicle that goes against the established traffic regulations of the region. An autonomous vehicle’s system is typically designed to adhere strictly to traffic laws.</li> </ol> <p>The participants are encouraged to use open-source LLMs to answer the questions with <em>in-context learning</em> to explore the reasoning ability of LLMs in understanding user commands. The provided commands are for evaluation, while the participants are free to use any other data for training or come up with their own examples for prompt engineering.</p> <h4 id="evaluation">Evaluation</h4> <p>To use the provided code for evaluation, the output should be in the following format:</p> <p><code class="language-plaintext highlighter-rouge">&lt;command_id&gt; &lt;A1&gt; &lt;A2&gt; &lt;A3&gt; &lt;A4&gt; &lt;A5&gt; &lt;A6&gt; &lt;A7&gt; &lt;A8&gt;</code></p> <p>where <code class="language-plaintext highlighter-rouge">&lt;command_id&gt;</code> is the command numeric identifier, starting with 1. Command IDs have been provided in the CSV file in the data download link. <code class="language-plaintext highlighter-rouge">&lt;A1&gt;</code> to <code class="language-plaintext highlighter-rouge">&lt;A8&gt;</code> are the answers to the 8 questions, where 1 indicates “Yes” and 0 indicates “No”.</p> <p>We use two accuracy metrics for evaluation:</p> <ol> <li> <strong>Command-level accuracy</strong>: A command is considered correctly understood if all eight answers are correct.</li> <li> <strong>Question-level accuracy</strong>: Evaluation at the individual question level.</li> </ol> <hr> <h3 id="citation">Citation</h3> <p>If the code, datasets, and research behind this workshop inspire you, please cite our work:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@inproceedings{tang2023thma,
  title={THMA: tencent HD Map AI system for creating HD map annotations},
  author={Tang, Kun and Cao, Xu and Cao, Zhipeng and Zhou, Tong and Li, Erlong and Liu, Ao and Zou, Shengtao and Liu, Chang and Mei, Shuqi and Sizikova, Elena and Zheng, Chao},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={37},
  number={13},
  pages={15585--15593},
  year={2023}
}
</code></pre></div></div> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>@article{zheng2023hdmap,
  title={High-Definition Map Automatic Annotation System Based on Active Learning},
  author={Zheng, Chao and Cao, Xu and Tang, Kun and Cao, Zhipeng and Sizikova, Elena and Zhou, Tong and Li, Erlong and Liu, Ao and Zou, Shengtao and Yan, Xinrui and Mei, Shuqi},
  journal={AI Magazine},
  year={2023},
  publisher={Wiley Online Library}
}
</code></pre></div></div> </article> </div> </div> <footer class="fixed-bottom"> <div class="container mt-0"> © Copyright 2024 . Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.6.1/dist/js/bootstrap.bundle.min.js" integrity="sha256-fgLAgv7fyCGopR/gBNq2iW3ZKIdqIcyshnUULC4vex8=" crossorigin="anonymous"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@4/imagesloaded.pkgd.min.js"></script> <script defer src="/assets/js/masonry.js" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.8/dist/medium-zoom.min.js" integrity="sha256-7PhEpEWEW0XXQ0k6kQrPKwuoIomz8R8IYyuU1Qew4P8=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js"></script> <script defer src="https://unpkg.com/bootstrap-table@1.21.4/dist/bootstrap-table.min.js"></script> <script src="/assets/js/no_defer.js?d633890033921b33e0ceb13d22340a9c"></script> <script defer src="/assets/js/common.js?acdb9690d7641b2f8d40529018c71a01"></script> <script defer src="/assets/js/copy_code.js?c9d9dd48933de3831b3ee5ec9c209cac" type="text/javascript"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script type="text/javascript">window.MathJax={tex:{tags:"ams"}};</script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script> <script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script type="text/javascript">function progressBarSetup(){"max"in document.createElement("progress")?(initializeProgressElement(),$(document).on("scroll",function(){progressBar.attr({value:getCurrentScrollPosition()})}),$(window).on("resize",initializeProgressElement)):(resizeProgressBar(),$(document).on("scroll",resizeProgressBar),$(window).on("resize",resizeProgressBar))}function getCurrentScrollPosition(){return $(window).scrollTop()}function initializeProgressElement(){let e=$("#navbar").outerHeight(!0);$("body").css({"padding-top":e}),$("progress-container").css({"padding-top":e}),progressBar.css({top:e}),progressBar.attr({max:getDistanceToScroll(),value:getCurrentScrollPosition()})}function getDistanceToScroll(){return $(document).height()-$(window).height()}function resizeProgressBar(){progressBar.css({width:getWidthPercentage()+"%"})}function getWidthPercentage(){return getCurrentScrollPosition()/getDistanceToScroll()*100}const progressBar=$("#progress");window.onload=function(){setTimeout(progressBarSetup,50)};</script> </body> </html>